{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fb3ecdd",
   "metadata": {},
   "source": [
    "1,\n",
    "Difference between Simple and Multiple Linear Regression: Simple Linear Regression models the relationship with a single predictor as \"Outcome equals beta zero plus beta one times Predictor A.\" Multiple Linear Regression, on the other hand, includes multiple predictors, allowing it to account for additional variables, which improves model complexity and predictive accuracy. In this case, \"Outcome equals beta zero plus beta one times Predictor A plus beta two times Predictor B, and so on.\" This helps in reducing omitted variable bias and better explains variations in the outcome.\n",
    "\n",
    "Difference between Continuous and Indicator Variables in Simple Linear Regression: A continuous variable represents a range of values and influences the slope in the model, expressed as \"Outcome equals beta zero plus beta one times Continuous Variable.\" An indicator, or binary, variable shifts the outcome without changing the slope, typically representing group categories. This is expressed as \"Outcome equals beta zero plus beta one times one if Indicator.\" These linear forms allow the model to differentiate between groups by shifting intercepts.\n",
    "\n",
    "Effect of Adding an Indicator Variable in Multiple Linear Regression: Adding an indicator variable along with a continuous variable creates parallel lines for each group in the model, where each group has a different intercept but shares the same slope. This can be expressed as \"Outcome equals beta zero plus beta one times Continuous Variable plus beta two times one if Indicator.\" This adjustment helps capture distinct baseline levels across groups.\n",
    "\n",
    "Effect of Adding an Interaction Term Between Continuous and Indicator Variables: Introducing an interaction term enables the slope of the continuous predictor to vary by category, allowing each group to have its unique trend. This is expressed as \"Outcome equals beta zero plus beta one times Continuous Variable plus beta two times one if Indicator plus beta three times Continuous Variable times one if Indicator.\" This linear form is particularly useful when the effect of the continuous variable is expected to differ across groups.\n",
    "\n",
    "Behavior of Multiple Linear Regression with Indicator Variables from a Non-Binary Categorical Variable: For non-binary categorical variables, each level (except a baseline) is represented by an indicator variable, with k minus one indicators for k categories. This is expressed as \"Outcome equals beta zero plus beta one times one if Category One plus beta two times one if Category Two, and so on.\" This binary encoding allows the model to capture the effect of each category individually, using the baseline as a reference point for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7663bf9b",
   "metadata": {},
   "source": [
    "2,\n",
    "Using Two Formulas for Predictions with and without Interaction:\n",
    "\n",
    "Without interaction, the model assumes that each advertising medium (TV and online) has an independent effect on sales, and the total effect is the sum of these individual effects. The linear form would look like:\n",
    "\"Sales Revenue = beta zero + beta one times TV Spend + beta two times Online Spend.\"\n",
    "With interaction, the model includes an additional term to account for how TV and online ad spending may influence each other. Here, the linear form would be:\n",
    "\"Sales Revenue = beta zero + beta one times TV Spend + beta two times Online Spend + beta three times (TV Spend times Online Spend).\"\n",
    "Interpretation: The interaction term (beta three) shows how the combined effect of TV and online advertising differs from their individual effects. If there is no interaction, the effect of one predictor remains constant regardless of the level of the other. With interaction, the effect of one predictor varies depending on the other.\n",
    "Binary Variables (High vs. Low Budget):\n",
    "\n",
    "If the advertisement budgets are categorized as \"high\" or \"low\" (binary), we can use indicator variables instead of continuous ones. Here, the linear forms would be:\n",
    "Without interaction: \"Sales Revenue = beta zero + beta one times 1(High TV Spend) + beta two times 1(High Online Spend).\"\n",
    "With interaction: \"Sales Revenue = beta zero + beta one times 1(High TV Spend) + beta two times 1(High Online Spend) + beta three times (1(High TV Spend) times 1(High Online Spend)).\"\n",
    "Interpretation: In the binary setup, the interaction term shows whether having a high budget in both mediums creates an additional effect on sales that is more than just the sum of each medium's individual effect. Without the interaction term, the effect of each predictor is independent of the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48df806",
   "metadata": {},
   "source": [
    "3,\n",
    "Question 3: Logistic Regression for Binary Outcomes Using smf\n",
    "\n",
    "Since our outcome is categorical or binary, we will use logistic regression instead of multiple linear regression. Here’s how to proceed:\n",
    "\n",
    "Additive Specification without Interaction:\n",
    "\n",
    "This model assumes that each predictor’s effect on the outcome is independent.\n",
    "Formula: \"binary_outcome ~ continuous_variable1 + binary_variable1 + categorical_variable1\".\n",
    "Steps:\n",
    "First, import the necessary libraries, including pandas for data handling and statsmodels.formula.api (imported as smf) for model fitting.\n",
    "Load the dataset from the provided URL, filling in any missing values with the string 'None'.\n",
    "Create a binary outcome variable by transforming a categorical variable into a binary form (e.g., setting the outcome to 1 if a Pokémon’s type is \"Fire\" and 0 otherwise).\n",
    "Define the model formula as \"binary_outcome ~ Attack + Legendary + Defense + C(Generation)\", where \"Attack\" and \"Defense\" are continuous variables, \"Legendary\" is binary, and \"Generation\" is categorical (indicated by C()).\n",
    "Fit the logistic regression model using smf.logit with the specified formula and print the summary.\n",
    "Interpretation: Each predictor’s coefficient indicates its log-odds impact on the probability of the binary outcome (e.g., being \"Fire\" type). A positive coefficient suggests a higher likelihood of the outcome, while a negative coefficient suggests a lower likelihood.\n",
    "Synergistic Specification with Interaction:\n",
    "\n",
    "This model includes interactions, allowing the effect of one predictor to depend on another.\n",
    "Formula: \"binary_outcome ~ continuous_variable1 * continuous_variable2 + binary_variable1 * categorical_variable1\".\n",
    "Steps:\n",
    "Define an updated model formula that includes interaction terms. For example, \"binary_outcome ~ Attack * Defense + Legendary * C(Generation)\".\n",
    "Fit this interaction model using smf.logit with the updated formula and print the summary.\n",
    "Interpretation: Interaction terms reveal combined effects, such as how \"Attack\" and \"Defense\" together influence the probability of the outcome.\n",
    "Interpreting Logistic Regression Results:\n",
    "\n",
    "Logistic regression coefficients represent log-odds, which differ from linear regression interpretation.\n",
    "To simplify, interpret coefficients as indicators of the direction and strength of association: positive values increase the likelihood of the outcome, and negative values decrease it.\n",
    "Statistical Evidence for Predictors:\n",
    "\n",
    "Use the summary output from the fitted model to examine p-values and coefficient magnitudes, assessing the statistical significance and strength of each predictor.\n",
    "Visualizing Relationships:\n",
    "\n",
    "Although logistic regression doesn’t directly provide a line of best fit, we can plot fitted probabilities to illustrate predictor effects on the outcome, treating them similarly to linear regression coefficients for visualization purposes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759e919b",
   "metadata": {},
   "source": [
    "4,\n",
    "The statement that \"the model only explains 17.6% of the variability in the data\" (indicating a low R-squared) may seem contradictory to the statement that \"many of the coefficients are large with strong evidence against the null hypothesis of 'no effect'\" (indicating significant p-values). Here’s how to interpret this:\n",
    "\n",
    "R-squared measures the proportion of variance in the outcome explained by the predictors. A low R-squared (e.g., 17.6%) suggests that much of the variation in the outcome remains unexplained by the model. This does not mean the model is ineffective but rather that it explains only a part of the variation in the outcome.\n",
    "\n",
    "P-values assess the statistical significance of each predictor, indicating whether each variable has a measurable association with the outcome. Low p-values (e.g., less than 0.05) provide evidence against the null hypothesis of no effect, meaning these predictors have a statistically significant impact on the outcome when other variables are held constant.\n",
    "\n",
    "Interpreting Both Metrics Together:\n",
    "\n",
    "The low R-squared suggests that while the predictors significantly affect the outcome, other factors not included in the model likely explain much of the variation in the outcome.\n",
    "Significant p-values indicate that even though the model doesn't capture all variability, the included predictors have a meaningful and statistically significant relationship with the outcome.\n",
    "In summary, R-squared and p-values capture different aspects of model performance: R-squared focuses on overall explanatory power, while p-values measure the strength of individual predictor effects. These metrics can coexist, showing that individual predictors are statistically significant while the model explains a limited portion of the total outcome variability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d582b832",
   "metadata": {},
   "source": [
    "5,\n",
    "Question 5: Discussing Code Results on In-Sample and Out-of-Sample R-squared\n",
    "\n",
    "In this question, we examine the concept of model generalizability by comparing the in-sample and out-of-sample R-squared values. The in-sample R-squared measures how well the model fits the training data, while the out-of-sample R-squared assesses its performance on a test set, giving insights into how well the model generalizes to new data.\n",
    "\n",
    "In-Sample R-squared: This metric is calculated as the squared correlation between the observed outcomes and the fitted values (predictions) within the training data. It reflects how much of the variation in the training set is explained by the model. A high in-sample R-squared typically indicates a good fit to the data used for model training.\n",
    "\n",
    "Out-of-Sample R-squared: This metric is computed as the squared correlation between the actual outcomes and the model’s predictions on the test set. A lower out-of-sample R-squared compared to the in-sample R-squared may indicate overfitting; the model may perform well on the data it was trained on but poorly on new, unseen data.\n",
    "\n",
    "Model Generalizability:\n",
    "\n",
    "A significant drop in the out-of-sample R-squared compared to the in-sample R-squared suggests that the model is not generalizing well, likely due to overfitting.\n",
    "A similar R-squared value for both in-sample and out-of-sample data suggests the model is performing consistently and is more likely to generalize well to new data.\n",
    "Purpose of Train-Test Split: Splitting the data into training and testing sets allows us to evaluate the model’s ability to predict outcomes for new data. By comparing in-sample and out-of-sample R-squared values, we can confirm whether the model is robust and reliable beyond the dataset it was trained on.\n",
    "\n",
    "In summary, the comparison of in-sample and out-of-sample R-squared values provides a practical check for model overfitting and generalizability. High in-sample R-squared with a considerably lower out-of-sample R-squared indicates a lack of generalizability, while similar values suggest a well-generalized model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581830c6",
   "metadata": {},
   "source": [
    "6,\n",
    "The overfitting observed in model4 is due to model complexity relative to the dataset size. Model4’s linear form is overly complex, containing numerous interaction terms, which results in a large number of predictor variables in the design matrix. This leads to overfitting as the model “detects” patterns that are actually just noise specific to the training dataset and do not generalize well to new data.\n",
    "\n",
    "Multicollinearity: The complex design matrix for model4 introduces multicollinearity, where predictor variables are highly correlated. This issue is evident in the high condition number (a diagnostic measure of multicollinearity) of model4’s design matrix, which remains extremely large even after centering and scaling continuous variables. High multicollinearity inflates standard errors, making model predictions unstable and unreliable for generalization.\n",
    "\n",
    "Effect on Generalization: The high condition number indicates poor generalizability. When a model is overly complex and multicollinear, it fits the training data too closely, capturing noise rather than meaningful patterns. This lack of generalizability results in poor out-of-sample performance, as seen when predictions from model4 fail to apply reliably to testing data.\n",
    "\n",
    "Model Comparison with Simpler Models: In contrast, a simpler model (like model3) is less prone to multicollinearity and captures only essential predictive associations. This parsimonious approach enables model3 to generalize better to new data, showing that simpler models are often more robust in capturing real relationships within a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79be47ee",
   "metadata": {},
   "source": [
    "7，\n",
    "The progression from model3 to model7 represents iterative adjustments aimed at enhancing model generalizability and predictive accuracy while managing multicollinearity.\n",
    "\n",
    "Development from model3 to model4: Model4 introduced more predictor variables and interaction terms, increasing model complexity. However, this led to significant multicollinearity, indicated by a high condition number. This excessive complexity caused overfitting, as model4 captured noise specific to the training set, reducing generalizability.\n",
    "\n",
    "Model5 and Model6 Adjustments: To mitigate overfitting, model5 and model6 were developed by simplifying model4. These models reduced complexity by limiting interactions and focusing on significant predictors. Centering and scaling were applied to continuous variables, which helped lower multicollinearity as indicated by the reduced condition number. These modifications aimed to create a more parsimonious model, improving stability and generalizability.\n",
    "\n",
    "Final Model (Model7): Model7 further refined the model specification to balance predictive performance and generalizability. Centering and scaling continuous variables reduced the condition number to a manageable level (15.4), indicating that multicollinearity was no longer a major issue. Model7 leverages only essential predictive associations, offering improved out-of-sample predictions without the instability observed in overfitted models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e205e0e",
   "metadata": {},
   "source": [
    "8，\n",
    "This exercise involves running a loop to repeatedly split data, fit a model, and calculate the \"in-sample\" and \"out-of-sample\" R-squared values over multiple iterations. By avoiding a fixed random seed, we introduce variability in the train-test splits for each iteration, which simulates different sample conditions and provides insights into model stability and generalizability.\n",
    "\n",
    "Purpose of the For Loop: The loop iteratively trains the model on different random splits of the data, each time measuring how well it performs on both the training (in-sample) and testing (out-of-sample) datasets. By comparing these R-squared values across iterations, we can observe patterns indicating overfitting, underfitting, or generalizable performance.\n",
    "\n",
    "Interpreting In-Sample vs. Out-of-Sample R-squared:\n",
    "\n",
    "High In-Sample R-squared with significantly lower Out-of-Sample R-squared suggests overfitting; the model captures noise specific to the training data but fails to generalize.\n",
    "Consistently similar R-squared values for both in-sample and out-of-sample across iterations indicates a well-generalized model.\n",
    "Visualizing Results: The scatter plot of in-sample vs. out-of-sample R-squared values provides a visual representation of the model’s stability. Ideally, points should be close to the line y=x, indicating similar performance on both datasets. Large deviations from this line indicate instability, often due to overfitting.\n",
    "\n",
    "Significance of This Demonstration: This approach reveals how consistent or variable model performance is across different data samples, illustrating the importance of model generalizability. A model that consistently performs well out-of-sample across random splits is more reliable and robust, making it better suited for real-world predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d56877a",
   "metadata": {},
   "source": [
    "9，\n",
    "Model Complexity and Generalizability:\n",
    "\n",
    "Model6: Simpler and more interpretable, this model is less prone to overfitting, as it does not capture noise or specific idiosyncrasies of the training data. Its simpler structure often leads to stronger out-of-sample generalizability.\n",
    "Model7: More complex, model7_fit has a higher risk of overfitting by capturing nuances specific to the training dataset. While it may show slightly improved in-sample performance, this often does not generalize well to new data, as evidenced by weaker p-values and inconsistent out-of-sample predictions.\n",
    "p-Values and Evidence:\n",
    "\n",
    "When examining the p-values of model7_fit, we find that the evidence supporting the significance of many estimated coefficients is not very strong. This implies that some relationships detected by model7_fit may be spurious, weakening its predictive power on unseen data.\n",
    "In contrast, model6_fit shows consistently stronger evidence (lower p-values) across its coefficients, indicating more reliable predictive associations.\n",
    "Sequential Data and Prediction:\n",
    "\n",
    "The code highlights that in real-world applications, data often arrives sequentially rather than being available in a static format. Using the model to predict future data, rather than re-splitting the dataset, better reflects how predictions might be used in practice.\n",
    "This sequential approach reveals that model7_fit struggles with future predictions, while model6_fit shows greater robustness, suggesting that simpler models may better handle generalizability in evolving datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
